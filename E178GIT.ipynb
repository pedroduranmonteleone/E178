import numpy as np
from astropy.table import Table
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import numpy as np
import csv
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.stats import pearsonr
from kneed import KneeLocator
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from astropy.io import fits



    # Read the data from the FITS file. Change the directory according to the dataset you are using. 
filename = 'C:/Users/pedro/Downloads/StellarMasses.fits'
table = Table.read(filename)

n_datapoints = len(table)
print(f'The dataset contains {n_datapoints} datapoints.')

# Convert the table to a pandas datafram. This basically makes the code "read" the dataset as an excel table
df = table.to_pandas()

#drop redundant features. In this dataset specifically, there data about the absolute magnitude of stars and then
#their corrections. Since it is redundant data, we filter and drop these a-like features.
#We will have to either delete or change this line of code in order to make it work for other datasets. 
redundant_cols = df.filter(like='absmag_').columns.difference(df.filter(like='absmag_*_stars').columns)
df.drop(redundant_cols, axis=1, inplace=True)

# Select only the numerical columns for normalization
#This is crucial. We select the proper columns for normalization
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
numeric_data = df[numeric_cols]

#Normalize the data using MinMaxScaler. Do this for every dataset we test!
scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(numeric_data)
    
normalized_df = pd.DataFrame(normalized_data, columns=numeric_cols)

    # Calculate the correlation coefficient between each feature and the redshift
    #Since some tables name the redshift column 'Z' differently, we may need to adjust the rest
    #of the code.
correlations = normalized_df.corr()['Z']


    # Select the top features with the highest absolute correlation coefficient
n_features = 100  
top_features = abs(correlations).nlargest(n_features).index.tolist()

    # Print the names of the top features
    #This hints us which features are best to use! Try combining the top choices differently,
    #sometimes five features make the score better, sometimes two make it better... 
    #We have to try out to find out!
print(f'Top {n_features} features that correlate with redshift:')
for feature in top_features:
    print(feature)



    # Select the features for clustering
    #Here you write your features that you want to try 
features = ['S2N', 'dellogLWage', 'dellogmoverl_i', 'nQ', 'PPP','Z','logmstar']

    #Create a new dataframe with only the selected features
    #Here we create another "Table" in order to separate the target variable 'Z' from the other
    #features. If we kept Z in the dataset, the bias of the linear regression would be bigger. 
X = normalized_df[features]
    #Separate target variable from other features
X = X.drop('Z', axis=1)
Y=normalized_df[features]['Z']
print(f"This is Z {Y}" )

    # Set the number of clusters
n_clusters = 3
    # Perform k-means clustering
kmeans = KMeans(n_clusters=n_clusters)
kmeans.fit(X)
normalized_df['cluster'] = kmeans.labels_




# Select the features and split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    # Train a linear regression model on the selected features
lr = LinearRegression()
lr.fit(X_train, y_train)

    # Evaluate the model on the testing set
score = lr.score(X_test, y_test)
print(f"Model accuracy: {score}")
